# batch-etl-pipeline
DE project

# Batch ETL Pipeline (Airflow + PySpark + Snowflake)

## ğŸš€ Overview
This project implements a production-style Batch ETL pipeline using:

- **Apache Airflow** for orchestration  
- **PySpark** for distributed data transformations  
- **Snowflake** as the cloud data warehouse  

The pipeline runs daily and performs:

1. **Extract**  
   - Downloads raw CSV/API data  
   - Stores it in `data/raw/`

2. **Transform**  
   - Cleans schema using PySpark  
   - Validates column types  
   - Handles missing values  
   - Writes processed files to `data/processed/`

3. **Load**  
   - Uploads processed data into Snowflake  
   - Uses Snowflake connector for Python or Spark connector  

---

## ğŸ§± Architecture

      Airflow DAG
          â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                â”‚
  Extract Task Transform Task (Spark)
  â”‚                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
       Load Task
          â”‚
    Snowflake Table


---

## ğŸ“ Repository Structure

batch-etl-pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ batch_etl_dag.py
â”œâ”€â”€ spark_jobs/
â”‚ â”œâ”€â”€ extract_data.py
â”‚ â”œâ”€â”€ transform_data.py
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ create_tables.sql
â”‚ â””â”€â”€ load_to_snowflake.sql
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ airflow_variables.json
â”‚ â””â”€â”€ snowflake_config.json
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/

---

## ğŸ”§ Tech Stack

- **Airflow** â€” Scheduling, orchestration  
- **PySpark** â€” Scalable transformations  
- **Snowflake** â€” Cloud data warehouse  
- **Docker** â€” Local Airflow environment  
- **Python / SQL**  

---

## ğŸ“Œ Next Steps
# - Add data quality checks (Great Expectations)  
# - Add CI/CD pipeline  
# - Add monitoring for pipeline failures  
-# Convert to a feature pipeline for ML  