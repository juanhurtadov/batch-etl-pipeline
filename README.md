# batch-etl-pipeline
DE project

# Batch ETL Pipeline (Airflow + PySpark + Snowflake)

## ğŸš€ Overview
This project implements a production-style Batch ETL pipeline using:

- **Apache Airflow** for orchestration  
- **PySpark** for distributed data transformations  
- **Snowflake** as the cloud data warehouse  

The pipeline runs daily and performs:

1. **Extract**  
   - Downloads raw CSV/API data  
   - Stores it in `data/raw/`

2. **Transform**  
   - Cleans schema using PySpark  
   - Validates column types  
   - Handles missing values  
   - Writes processed files to `data/processed/`

3. **Load**  
   - Uploads processed data into Snowflake  
   - Uses Snowflake connector for Python or Spark connector  

---

## ğŸ§± Architecture

          Airflow DAG
              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                â”‚
      Extract Task     Transform Task (Spark)
      â”‚                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
         Load Task
              â”‚
        Snowflake Table

## ğŸ“ Repository Structure

batch-etl-pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ batch_etl_dag.py
â”‚
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â””â”€â”€ transform_data.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ load_to_snowflake.sql
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ airflow_variables.json
â”‚   â””â”€â”€ snowflake_config.json
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â””â”€â”€ README.md


## ğŸ”§ Tech Stack

- **Airflow** â€” Scheduling, orchestration  
- **PySpark** â€” Scalable transformations  
- **Snowflake** â€” Cloud data warehouse  
- **Docker** â€” Local Airflow environment  
- **Python / SQL**  
