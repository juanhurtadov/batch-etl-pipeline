# batch-etl-pipeline
DE project

# Batch ETL Pipeline (Airflow + PySpark + Snowflake)

## ğŸš€ Overview
This project implements a production-style Batch ETL pipeline using:

- **Apache Airflow** for orchestration  
- **PySpark** for distributed data transformations  
- **Snowflake** as the cloud data warehouse  

The pipeline runs daily and performs:

1. **Extract**  
   - Downloads raw CSV/API data  
   - Stores it in `data/raw/`

2. **Transform**  
   - Cleans schema using PySpark  
   - Validates column types  
   - Handles missing values  
   - Writes processed files to `data/processed/`

3. **Load**  
   - Uploads processed data into Snowflake  
   - Uses Snowflake connector for Python or Spark connector  

---

## ğŸ§± Architecture

          Airflow DAG
              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                â”‚
 Extract Task     Transform Task (Spark)
      â”‚                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
         Load Task
              â”‚
        Snowflake Table

## ğŸ“ Repository Structure

batch-etl-pipeline/
â”œâ”€â”€ dags/
â”œâ”€â”€ spark_jobs/
â”œâ”€â”€ sql/
â”œâ”€â”€ config/
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ docker/
â””â”€â”€ README.md

## ğŸ”§ Tech Stack

- **Airflow** â€” Scheduling, orchestration  
- **PySpark** â€” Scalable transformations  
- **Snowflake** â€” Cloud data warehouse  
- **Docker** â€” Local Airflow environment  
- **Python / SQL**  
