# batch-etl-pipeline
DE project

# Batch ETL Pipeline (Airflow + PySpark + Snowflake)

## ğŸš€ Overview
This project implements a production-style Batch ETL pipeline using:

- **Apache Airflow** for orchestration  
- **PySpark** for distributed data transformations  
- **Snowflake** as the cloud data warehouse  

The pipeline runs daily and performs:

1. **Extract**  
   - Downloads raw CSV/API data  
   - Stores it in `data/raw/`

2. **Transform**  
   - Cleans schema using PySpark  
   - Validates column types  
   - Handles missing values  
   - Writes processed files to `data/processed/`

3. **Load**  
   - Uploads processed data into Snowflake  
   - Uses Snowflake connector for Python or Spark connector  

---

## ğŸ§± Architecture

flowchart TD
    A[Airflow DAG] --> B[Extract Task]
    A --> C[Transform Task (Spark)]
    B --> D[Load Task]
    C --> D
    D --> E[Snowflake Table]

## ğŸ“ Repository Structure

batch-etl-pipeline/
â”œâ”€â”€ dags/
â”œâ”€â”€ spark_jobs/
â”œâ”€â”€ sql/
â”œâ”€â”€ config/
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ docker/
â””â”€â”€ README.md

## ğŸ”§ Tech Stack

- **Airflow** â€” Scheduling, orchestration  
- **PySpark** â€” Scalable transformations  
- **Snowflake** â€” Cloud data warehouse  
- **Docker** â€” Local Airflow environment  
- **Python / SQL**  
